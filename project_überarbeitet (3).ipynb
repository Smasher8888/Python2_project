{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Celin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Celin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Celin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Celin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import requests\n",
    "import time\n",
    "\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting data from yahoo finance\n",
    "\n",
    "import yfinance as yf\n",
    "nasdaq = yf.Ticker(\"^IXIC\")\n",
    "\n",
    "#getting data for first half of year 2022\n",
    "df_nasdaq = nasdaq.history(start = \"2022-01-01\", end = \"2022-12-31\")\n",
    "\n",
    "#daily labels\n",
    "\n",
    "df_labels = pd.DataFrame() \n",
    "df_labels[\"Open-Close\"] = df_nasdaq['Open'] - df_nasdaq[\"Close\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing time of day\n",
    "df_nasdaq.index = df_nasdaq.index.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-d0fb6ff2ee21>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_labels['label'][i] = 0\n",
      "<ipython-input-4-d0fb6ff2ee21>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_labels['label'][i] = 1\n"
     ]
    }
   ],
   "source": [
    "#creating label for each day\n",
    "df_labels['label'] = 'dummy'\n",
    "for i in range(len(df_labels)):\n",
    "    if df_labels['Open-Close'][i] < 0:\n",
    "        df_labels['label'][i] = 0\n",
    "    elif df_labels['Open-Close'][i] > 0:\n",
    "        df_labels['label'][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label for each day \n",
    "y = df_labels['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove first day\n",
    "y = y.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"wwFVxDThbk6roswX4uS8aA7zC7Akuqt0\"\n",
    "year = 2022\n",
    "articles = []\n",
    "\n",
    "#split the request into quarters because of too large requests\n",
    "\n",
    "for month in range(1, 4):\n",
    "    url = f\"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={api_key}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Check if the response was successful\n",
    "    if response.status_code != 200:\n",
    "        continue\n",
    "\n",
    "    # Check if 'response' key exists in the data\n",
    "    if 'response' not in data:\n",
    "        continue\n",
    "\n",
    "    articles.extend(data['response']['docs']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in range(4, 7):\n",
    "    url = f\"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={api_key}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Check if the response was successful\n",
    "    if response.status_code != 200:\n",
    "        continue\n",
    "\n",
    "    # Check if 'response' key exists in the data\n",
    "    if 'response' not in data:\n",
    "        continue\n",
    "\n",
    "    articles.extend(data['response']['docs']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in range(7, 10):\n",
    "    url = f\"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={api_key}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Check if the response was successful\n",
    "    if response.status_code != 200:\n",
    "        continue\n",
    "\n",
    "    # Check if 'response' key exists in the data\n",
    "    if 'response' not in data:\n",
    "        continue\n",
    "\n",
    "    articles.extend(data['response']['docs']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in range(10, 13):\n",
    "    url = f\"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={api_key}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Check if the response was successful\n",
    "    if response.status_code != 200:\n",
    "        continue\n",
    "\n",
    "    # Check if 'response' key exists in the data\n",
    "    if 'response' not in data:\n",
    "        continue\n",
    "\n",
    "    articles.extend(data['response']['docs']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>web_url</th>\n",
       "      <th>snippet</th>\n",
       "      <th>lead_paragraph</th>\n",
       "      <th>print_section</th>\n",
       "      <th>print_page</th>\n",
       "      <th>source</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>document_type</th>\n",
       "      <th>news_desk</th>\n",
       "      <th>section_name</th>\n",
       "      <th>subsection_name</th>\n",
       "      <th>byline</th>\n",
       "      <th>type_of_material</th>\n",
       "      <th>_id</th>\n",
       "      <th>word_count</th>\n",
       "      <th>uri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48702</th>\n",
       "      <td>She infused her work with political and femini...</td>\n",
       "      <td>https://www.nytimes.com/2022/12/31/obituaries/...</td>\n",
       "      <td>She infused her work with political and femini...</td>\n",
       "      <td>Jean Franco, a pioneering literary scholar who...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>[{'rank': 0, 'subtype': 'xlarge', 'caption': N...</td>\n",
       "      <td>{'main': 'Jean Franco, 98, Pioneering Scholar ...</td>\n",
       "      <td>[{'name': 'subject', 'value': 'Colleges and Un...</td>\n",
       "      <td>2022-12-31T19:10:36+0000</td>\n",
       "      <td>article</td>\n",
       "      <td>Obits</td>\n",
       "      <td>Obituaries</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'original': 'By Clay Risen', 'person': [{'fir...</td>\n",
       "      <td>News</td>\n",
       "      <td>nyt://article/4da4f196-7b40-57ee-ad72-2e0c74ef...</td>\n",
       "      <td>969</td>\n",
       "      <td>nyt://article/4da4f196-7b40-57ee-ad72-2e0c74ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48703</th>\n",
       "      <td>Heavy rain, snow and wind were battering parts...</td>\n",
       "      <td>https://www.nytimes.com/2022/12/31/us/californ...</td>\n",
       "      <td>Heavy rain, snow and wind were battering parts...</td>\n",
       "      <td>Heavy rain and snow caused landslides and floo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>[{'rank': 0, 'subtype': 'xlarge', 'caption': N...</td>\n",
       "      <td>{'main': 'Extreme Weather in California Causes...</td>\n",
       "      <td>[{'name': 'subject', 'value': 'Floods', 'rank'...</td>\n",
       "      <td>2022-12-31T19:21:09+0000</td>\n",
       "      <td>article</td>\n",
       "      <td>Express</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'original': 'By April Rubin', 'person': [{'fi...</td>\n",
       "      <td>News</td>\n",
       "      <td>nyt://article/5bd9694a-20ef-5f9a-9b32-bb1b829a...</td>\n",
       "      <td>530</td>\n",
       "      <td>nyt://article/5bd9694a-20ef-5f9a-9b32-bb1b829a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48704</th>\n",
       "      <td>Each year, the comic headlined the club during...</td>\n",
       "      <td>https://www.nytimes.com/2022/12/31/arts/televi...</td>\n",
       "      <td>Each year, the comic headlined the club during...</td>\n",
       "      <td>How do you honor the death of a comedy club? F...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>[{'rank': 0, 'subtype': 'xlarge', 'caption': N...</td>\n",
       "      <td>{'main': 'Dave Attell Bids a Heartfelt (and Hi...</td>\n",
       "      <td>[{'name': 'subject', 'value': 'Comedy and Humo...</td>\n",
       "      <td>2022-12-31T20:18:43+0000</td>\n",
       "      <td>article</td>\n",
       "      <td>Culture</td>\n",
       "      <td>Arts</td>\n",
       "      <td>Television</td>\n",
       "      <td>{'original': 'By Jason Zinoman and Photographs...</td>\n",
       "      <td>News</td>\n",
       "      <td>nyt://article/dae0c45e-3db9-5a73-b711-a08e94dc...</td>\n",
       "      <td>1163</td>\n",
       "      <td>nyt://article/dae0c45e-3db9-5a73-b711-a08e94dc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48705</th>\n",
       "      <td>What brought the owl to the city of Cypress, i...</td>\n",
       "      <td>https://www.nytimes.com/2022/12/31/us/snowy-ow...</td>\n",
       "      <td>What brought the owl to the city of Cypress, i...</td>\n",
       "      <td>The forbidding frozen wilderness of the high A...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>[{'rank': 0, 'subtype': 'xlarge', 'caption': N...</td>\n",
       "      <td>{'main': '‘Extremely Rare’ Snowy Owl Sighting ...</td>\n",
       "      <td>[{'name': 'subject', 'value': 'Owls', 'rank': ...</td>\n",
       "      <td>2022-12-31T20:22:56+0000</td>\n",
       "      <td>article</td>\n",
       "      <td>Express</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'original': 'By Michael Levenson', 'person': ...</td>\n",
       "      <td>News</td>\n",
       "      <td>nyt://article/86e3ee7f-db58-5517-a18b-dbcf78ce...</td>\n",
       "      <td>653</td>\n",
       "      <td>nyt://article/86e3ee7f-db58-5517-a18b-dbcf78ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48706</th>\n",
       "      <td>Answering reader queries about the state of th...</td>\n",
       "      <td>https://www.nytimes.com/2022/12/31/arts/music/...</td>\n",
       "      <td>Answering reader queries about the state of th...</td>\n",
       "      <td>Each year, oodles of questions pour in from th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>[{'rank': 0, 'subtype': 'xlarge', 'caption': N...</td>\n",
       "      <td>{'main': 'Year-End Listener Mailbag: Your 2022...</td>\n",
       "      <td>[{'name': 'subject', 'value': 'audio-neutral-i...</td>\n",
       "      <td>2022-12-31T20:46:45+0000</td>\n",
       "      <td>article</td>\n",
       "      <td>Culture</td>\n",
       "      <td>Arts</td>\n",
       "      <td>Music</td>\n",
       "      <td>{'original': '', 'person': [], 'organization':...</td>\n",
       "      <td>News</td>\n",
       "      <td>nyt://article/f27da0fd-710f-525f-a10a-05c14dfd...</td>\n",
       "      <td>141</td>\n",
       "      <td>nyt://article/f27da0fd-710f-525f-a10a-05c14dfd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstract  \\\n",
       "48702  She infused her work with political and femini...   \n",
       "48703  Heavy rain, snow and wind were battering parts...   \n",
       "48704  Each year, the comic headlined the club during...   \n",
       "48705  What brought the owl to the city of Cypress, i...   \n",
       "48706  Answering reader queries about the state of th...   \n",
       "\n",
       "                                                 web_url  \\\n",
       "48702  https://www.nytimes.com/2022/12/31/obituaries/...   \n",
       "48703  https://www.nytimes.com/2022/12/31/us/californ...   \n",
       "48704  https://www.nytimes.com/2022/12/31/arts/televi...   \n",
       "48705  https://www.nytimes.com/2022/12/31/us/snowy-ow...   \n",
       "48706  https://www.nytimes.com/2022/12/31/arts/music/...   \n",
       "\n",
       "                                                 snippet  \\\n",
       "48702  She infused her work with political and femini...   \n",
       "48703  Heavy rain, snow and wind were battering parts...   \n",
       "48704  Each year, the comic headlined the club during...   \n",
       "48705  What brought the owl to the city of Cypress, i...   \n",
       "48706  Answering reader queries about the state of th...   \n",
       "\n",
       "                                          lead_paragraph print_section  \\\n",
       "48702  Jean Franco, a pioneering literary scholar who...           NaN   \n",
       "48703  Heavy rain and snow caused landslides and floo...           NaN   \n",
       "48704  How do you honor the death of a comedy club? F...           NaN   \n",
       "48705  The forbidding frozen wilderness of the high A...           NaN   \n",
       "48706  Each year, oodles of questions pour in from th...           NaN   \n",
       "\n",
       "      print_page              source  \\\n",
       "48702        NaN  The New York Times   \n",
       "48703        NaN  The New York Times   \n",
       "48704        NaN  The New York Times   \n",
       "48705        NaN  The New York Times   \n",
       "48706        NaN  The New York Times   \n",
       "\n",
       "                                              multimedia  \\\n",
       "48702  [{'rank': 0, 'subtype': 'xlarge', 'caption': N...   \n",
       "48703  [{'rank': 0, 'subtype': 'xlarge', 'caption': N...   \n",
       "48704  [{'rank': 0, 'subtype': 'xlarge', 'caption': N...   \n",
       "48705  [{'rank': 0, 'subtype': 'xlarge', 'caption': N...   \n",
       "48706  [{'rank': 0, 'subtype': 'xlarge', 'caption': N...   \n",
       "\n",
       "                                                headline  \\\n",
       "48702  {'main': 'Jean Franco, 98, Pioneering Scholar ...   \n",
       "48703  {'main': 'Extreme Weather in California Causes...   \n",
       "48704  {'main': 'Dave Attell Bids a Heartfelt (and Hi...   \n",
       "48705  {'main': '‘Extremely Rare’ Snowy Owl Sighting ...   \n",
       "48706  {'main': 'Year-End Listener Mailbag: Your 2022...   \n",
       "\n",
       "                                                keywords  \\\n",
       "48702  [{'name': 'subject', 'value': 'Colleges and Un...   \n",
       "48703  [{'name': 'subject', 'value': 'Floods', 'rank'...   \n",
       "48704  [{'name': 'subject', 'value': 'Comedy and Humo...   \n",
       "48705  [{'name': 'subject', 'value': 'Owls', 'rank': ...   \n",
       "48706  [{'name': 'subject', 'value': 'audio-neutral-i...   \n",
       "\n",
       "                       pub_date document_type news_desk section_name  \\\n",
       "48702  2022-12-31T19:10:36+0000       article     Obits   Obituaries   \n",
       "48703  2022-12-31T19:21:09+0000       article   Express         U.S.   \n",
       "48704  2022-12-31T20:18:43+0000       article   Culture         Arts   \n",
       "48705  2022-12-31T20:22:56+0000       article   Express         U.S.   \n",
       "48706  2022-12-31T20:46:45+0000       article   Culture         Arts   \n",
       "\n",
       "      subsection_name                                             byline  \\\n",
       "48702             NaN  {'original': 'By Clay Risen', 'person': [{'fir...   \n",
       "48703             NaN  {'original': 'By April Rubin', 'person': [{'fi...   \n",
       "48704      Television  {'original': 'By Jason Zinoman and Photographs...   \n",
       "48705             NaN  {'original': 'By Michael Levenson', 'person': ...   \n",
       "48706           Music  {'original': '', 'person': [], 'organization':...   \n",
       "\n",
       "      type_of_material                                                _id  \\\n",
       "48702             News  nyt://article/4da4f196-7b40-57ee-ad72-2e0c74ef...   \n",
       "48703             News  nyt://article/5bd9694a-20ef-5f9a-9b32-bb1b829a...   \n",
       "48704             News  nyt://article/dae0c45e-3db9-5a73-b711-a08e94dc...   \n",
       "48705             News  nyt://article/86e3ee7f-db58-5517-a18b-dbcf78ce...   \n",
       "48706             News  nyt://article/f27da0fd-710f-525f-a10a-05c14dfd...   \n",
       "\n",
       "       word_count                                                uri  \n",
       "48702         969  nyt://article/4da4f196-7b40-57ee-ad72-2e0c74ef...  \n",
       "48703         530  nyt://article/5bd9694a-20ef-5f9a-9b32-bb1b829a...  \n",
       "48704        1163  nyt://article/dae0c45e-3db9-5a73-b711-a08e94dc...  \n",
       "48705         653  nyt://article/86e3ee7f-db58-5517-a18b-dbcf78ce...  \n",
       "48706         141  nyt://article/f27da0fd-710f-525f-a10a-05c14dfd...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_list = []\n",
    "# Process the articles\n",
    "for article in articles:\n",
    "    headline = article['headline']['main']\n",
    "    lead_paragraph = article['lead_paragraph']\n",
    "    section_name = article['section_name']\n",
    "    date = article['pub_date']        \n",
    "        # Tokenize the words in the lead paragraph\n",
    "    space_tokenizer = nltk.tokenize.SpaceTokenizer()\n",
    "    tokenized_paragraph = space_tokenizer.tokenize(lead_paragraph)\n",
    "        \n",
    "        # Convert words to lowercase   \n",
    "    tokenized_paragraph_lower = [w.lower() for w in tokenized_paragraph]\n",
    "        \n",
    "        # Remove stopwords from the tokenized paragraph\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens_without_stopwords = [w for w in tokenized_paragraph_lower if w not in stop_words]\n",
    "    tokens_without_stopwords = [token for token in tokens_without_stopwords if len(token) > 0]\n",
    "        \n",
    "        # POS Tag the token\n",
    "    paragraph_processed = nltk.pos_tag(tokens_without_stopwords)\n",
    "        \n",
    "        # Lemmatize the token\n",
    "    wordnet = nltk.WordNetLemmatizer()\n",
    "        #tokens_lemmatized = [wordnet.lemmatize(t) for t in tokenized_paragraph_lower]\n",
    "        \n",
    "    articles_list.append({\n",
    "        \"headline\": headline,\n",
    "        \"lead_paragraph\": lead_paragraph,\n",
    "        \"section_name\": section_name,\n",
    "         #   \"tokens_lemmad\": tokens_lemmatized,\n",
    "        \"processed_paragraph\": paragraph_processed,\n",
    "        'date': date[:10]\n",
    "        })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.DataFrame.from_records(articles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = articles_df.drop('section_name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>lead_paragraph</th>\n",
       "      <th>processed_paragraph</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48702</th>\n",
       "      <td>Jean Franco, 98, Pioneering Scholar of Latin A...</td>\n",
       "      <td>Jean Franco, a pioneering literary scholar who...</td>\n",
       "      <td>[(jean, JJ), (franco,, NN), (pioneering, VBG),...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48703</th>\n",
       "      <td>Extreme Weather in California Causes Flooding ...</td>\n",
       "      <td>Heavy rain and snow caused landslides and floo...</td>\n",
       "      <td>[(heavy, JJ), (rain, NN), (snow, NNS), (caused...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48704</th>\n",
       "      <td>Dave Attell Bids a Heartfelt (and Hilarious) F...</td>\n",
       "      <td>How do you honor the death of a comedy club? F...</td>\n",
       "      <td>[(honor, JJ), (death, NN), (comedy, NN), (club...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48705</th>\n",
       "      <td>‘Extremely Rare’ Snowy Owl Sighting Transfixes...</td>\n",
       "      <td>The forbidding frozen wilderness of the high A...</td>\n",
       "      <td>[(forbidding, VBG), (frozen, JJ), (wilderness,...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48706</th>\n",
       "      <td>Year-End Listener Mailbag: Your 2022 Questions...</td>\n",
       "      <td>Each year, oodles of questions pour in from th...</td>\n",
       "      <td>[(year,, NN), (oodles, NNS), (questions, NNS),...</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  \\\n",
       "48702  Jean Franco, 98, Pioneering Scholar of Latin A...   \n",
       "48703  Extreme Weather in California Causes Flooding ...   \n",
       "48704  Dave Attell Bids a Heartfelt (and Hilarious) F...   \n",
       "48705  ‘Extremely Rare’ Snowy Owl Sighting Transfixes...   \n",
       "48706  Year-End Listener Mailbag: Your 2022 Questions...   \n",
       "\n",
       "                                          lead_paragraph  \\\n",
       "48702  Jean Franco, a pioneering literary scholar who...   \n",
       "48703  Heavy rain and snow caused landslides and floo...   \n",
       "48704  How do you honor the death of a comedy club? F...   \n",
       "48705  The forbidding frozen wilderness of the high A...   \n",
       "48706  Each year, oodles of questions pour in from th...   \n",
       "\n",
       "                                     processed_paragraph        date  \n",
       "48702  [(jean, JJ), (franco,, NN), (pioneering, VBG),...  2022-12-31  \n",
       "48703  [(heavy, JJ), (rain, NN), (snow, NNS), (caused...  2022-12-31  \n",
       "48704  [(honor, JJ), (death, NN), (comedy, NN), (club...  2022-12-31  \n",
       "48705  [(forbidding, VBG), (frozen, JJ), (wilderness,...  2022-12-31  \n",
       "48706  [(year,, NN), (oodles, NNS), (questions, NNS),...  2022-12-31  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping headlines and paragraph for each day\n",
    "grouped_df = articles_df.groupby('date').agg({'headline': lambda x: ' '.join(x), 'lead_paragraph': lambda x: ' '.join(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming index to datetime\n",
    "from datetime import datetime\n",
    "\n",
    "dates = []\n",
    "# Example with the standard date and time format\n",
    "for i in range(len(grouped_df)):\n",
    "    date_str = grouped_df.index[i]\n",
    "    date_format = '%Y-%m-%d'\n",
    "    date_obj = datetime.strptime(date_str, date_format)\n",
    "    dates.append(date_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the same date format as in the label\n",
    "grouped_df.index = dates\n",
    "grouped_df.index = grouped_df.index.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only extract the articles that were published on dates that theres also data for the labels\n",
    "\n",
    "grouped_df = grouped_df.loc[df_nasdaq.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment scores\n",
    "#sentiments each get a score between 0 and 1 \n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "#headlines\n",
    "neg = []\n",
    "pos = []\n",
    "neu = []\n",
    "compound = []\n",
    "for sentence in grouped_df[\"headline\"]:\n",
    "    polarity_scores = analyzer.polarity_scores(sentence)\n",
    "    neg.append(polarity_scores['neg'])\n",
    "    pos.append(polarity_scores['pos'])\n",
    "    neu.append(polarity_scores['neu'])\n",
    "    compound.append(polarity_scores['compound'])\n",
    "\n",
    "neg = np.array(neg)[..., None]\n",
    "neu = np.array(neu)[..., None]\n",
    "pos = np.array(pos)[..., None]\n",
    "compound = np.array(compound)[..., None]\n",
    "\n",
    "#create feature matrix for headlines\n",
    "X_sentiment_detailed_headlines = np.concatenate((neg, neu, pos, compound), axis = 1)\n",
    "\n",
    "#lead paragraphs\n",
    "\n",
    "neg_para = []\n",
    "pos_para = []\n",
    "neu_para = []\n",
    "compound_para = []\n",
    "for sentence in grouped_df[\"lead_paragraph\"]:\n",
    "    polarity_scores = analyzer.polarity_scores(sentence)\n",
    "    neg_para.append(polarity_scores['neg'])\n",
    "    pos_para.append(polarity_scores['pos'])\n",
    "    neu_para.append(polarity_scores['neu'])\n",
    "    compound_para.append(polarity_scores['compound'])\n",
    "\n",
    "neg_para = np.array(neg_para)[..., None]\n",
    "neu_para = np.array(neu_para)[..., None]\n",
    "pos_para = np.array(pos_para)[..., None]\n",
    "compound_para = np.array(compound_para)[..., None]\n",
    "\n",
    "#create feature matrix for paragraphs\n",
    "X_sentiment_detailed_paragraphs = np.concatenate((neg_para, neu_para, pos_para, compound_para), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove last day\n",
    "X_sentiment_detailed_headlines = X_sentiment_detailed_headlines[:-1]\n",
    "X_sentiment_detailed_paragraphs = X_sentiment_detailed_paragraphs[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 4)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sentiment_detailed_paragraphs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Named entity features \n",
    "#headlines\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "text1_headlines = [NER(string) for string in grouped_df[\"headline\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#headlines\n",
    "labels_headlines = []\n",
    "for text in text1_headlines:\n",
    "    labels = []\n",
    "    for word in text.ents:\n",
    "        labels.append(word.label_)\n",
    "    labels_headlines.append(' '.join(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()\n",
    "bag = count.fit_transform(labels_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(251, 18)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized feature matrix \n",
    "X_topic_label_feature_headlines = bag/np.max(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove last day\n",
    "X_topic_label_feature_headlines = X_topic_label_feature_headlines[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lead paragraph\n",
    "text1_paragraphs = [NER(string) for string in grouped_df[\"lead_paragraph\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_paragraph = []\n",
    "for text in text1_paragraphs:\n",
    "    labels = []\n",
    "    for word in text.ents:\n",
    "        labels.append(word.label_)\n",
    "    labels_paragraph.append(' '.join(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_paragraphs = count.fit_transform(labels_paragraph).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized feature matrix\n",
    "X_topic_label_feature_paragraph = bag_paragraphs/np.max(bag_paragraphs)\n",
    "X_topic_label_feature_paragraph = X_topic_label_feature_paragraph[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 44)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat all features \n",
    "df_features = np.concatenate((X_sentiment_detailed_headlines, X_sentiment_detailed_paragraphs, X_topic_label_feature_paragraph, X_topic_label_feature_headlines), axis=1)\n",
    "df_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get labes from y\n",
    "y_labels = y.values\n",
    "y_labels = y_labels.astype(int)\n",
    "y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 118\n",
      "1: 132\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts = np.unique(y_labels, return_counts=True)\n",
    "for element, count in zip(unique_elements, counts):\n",
    "    print(f\"{element}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perceptron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 70, Best max_iter: 10, Best eta: 1e-26, Accuracy: 0.59\n",
      "Train size: 75, Best max_iter: 10, Best eta: 1e-26, Accuracy: 0.54\n",
      "Train size: 80, Best max_iter: 10, Best eta: 1e-26, Accuracy: 0.50\n",
      "Train size: 85, Best max_iter: 20, Best eta: 0.05, Accuracy: 0.45\n",
      "Train size: 90, Best max_iter: 10, Best eta: 1e-26, Accuracy: 0.40\n"
     ]
    }
   ],
   "source": [
    "# Create the Perceptron model\n",
    "ppn = Perceptron(random_state=1)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'max_iter': [10, 20, 30, 40, 50, 100, 500, 1000],  \n",
    "    'eta0': [0.00000000000000000000000001, 0.000000000001, 0.0000001, 0.000001, 0.01, 0.05, 0.1, 0.2]    \n",
    "}\n",
    "\n",
    "# Perform grid search and evaluation with different train-test splits\n",
    "for train_size in [70, 75, 80, 85, 90]:\n",
    "    \n",
    "    # Split the data into training and testing subsets\n",
    "    train_size_index = int(df_features.shape[0] * (train_size / 100))\n",
    "    X_train = df_features[:train_size_index]\n",
    "    X_test = df_features[train_size_index:]\n",
    "    y_train = y_labels[:train_size_index]\n",
    "    y_test = y_labels[train_size_index:]\n",
    "\n",
    "\n",
    "    # Create the GridSearchCV object\n",
    "    grid_search = GridSearchCV(ppn, param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "    # Perform the grid search on your data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters from the grid search\n",
    "    best_max_iter = grid_search.best_params_['max_iter']\n",
    "    best_eta = grid_search.best_params_['eta0']\n",
    "\n",
    "    # Train the final model with the best hyperparameters\n",
    "    final_ppn = Perceptron(max_iter=best_max_iter, eta0=best_eta, random_state=1)\n",
    "    final_ppn.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = final_ppn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Train size: {train_size}, Best max_iter: {best_max_iter}, Best eta: {best_eta}, Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 70, Best max_depth: 8, Best min_samples_split: 2, Best min_samples_leaf: 2, Accuracy: 0.57\n",
      "Train size: 75, Best max_depth: 6, Best min_samples_split: 2, Best min_samples_leaf: 2, Accuracy: 0.59\n",
      "Train size: 80, Best max_depth: 4, Best min_samples_split: 2, Best min_samples_leaf: 4, Accuracy: 0.66\n",
      "Train size: 85, Best max_depth: 6, Best min_samples_split: 10, Best min_samples_leaf: 4, Accuracy: 0.55\n",
      "Train size: 90, Best max_depth: 6, Best min_samples_split: 2, Best min_samples_leaf: 1, Accuracy: 0.56\n"
     ]
    }
   ],
   "source": [
    "# Create the DecisionTreeClassifier model\n",
    "tree_clf = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'max_depth': [2, 4, 6, 8],         # Different values for max_depth\n",
    "    'min_samples_split': [2, 5, 10],   # Different values for min_samples_split\n",
    "    'min_samples_leaf': [1, 2, 4]      # Different values for min_samples_leaf\n",
    "}\n",
    "\n",
    "# Perform grid search and evaluation with different train-test splits\n",
    "for train_size in [70, 75, 80, 85, 90]:\n",
    "    # Split the data into training and testing subsets\n",
    "    train_size_index = int(df_features.shape[0] * (train_size / 100))\n",
    "    X_train = df_features[:train_size_index]\n",
    "    X_test = df_features[train_size_index:]\n",
    "    y_train = y_labels[:train_size_index]\n",
    "    y_test = y_labels[train_size_index:]\n",
    "\n",
    "\n",
    "    # Create the GridSearchCV object\n",
    "    grid_search = GridSearchCV(tree_clf, param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "    # Perform the grid search on your data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters from the grid search\n",
    "    best_max_depth = grid_search.best_params_['max_depth']\n",
    "    best_min_samples_split = grid_search.best_params_['min_samples_split']\n",
    "    best_min_samples_leaf = grid_search.best_params_['min_samples_leaf']\n",
    "\n",
    "    # Train the final model with the best hyperparameters\n",
    "    final_tree_clf = DecisionTreeClassifier(\n",
    "        max_depth=best_max_depth,\n",
    "        min_samples_split=best_min_samples_split,\n",
    "        min_samples_leaf=best_min_samples_leaf,\n",
    "        random_state=1\n",
    "    )\n",
    "    final_tree_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    accuracy = final_tree_clf.score(X_test, y_test)\n",
    "    print(f\"Train size: {train_size}, Best max_depth: {best_max_depth}, Best min_samples_split: {best_min_samples_split}, Best min_samples_leaf: {best_min_samples_leaf}, Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 70, Best kernel: rbf, Best C: 1, Best gamma: auto, Accuracy: 0.40\n",
      "Train size: 75, Best kernel: linear, Best C: 0.001, Best gamma: scale, Accuracy: 0.57\n",
      "Train size: 80, Best kernel: linear, Best C: 10, Best gamma: scale, Accuracy: 0.54\n",
      "Train size: 85, Best kernel: linear, Best C: 0.001, Best gamma: scale, Accuracy: 0.55\n",
      "Train size: 90, Best kernel: linear, Best C: 0.001, Best gamma: scale, Accuracy: 0.60\n"
     ]
    }
   ],
   "source": [
    "# Create the SVM model\n",
    "svm_clf = SVC(random_state=42)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'rbf'],          # Different kernels to try\n",
    "    'C': [0.001, 0.1, 1, 10],             # Different values of C\n",
    "    'gamma': ['scale', 'auto', 0.1, 1]    # Different values of gamma (only for 'rbf' kernel)\n",
    "}\n",
    "\n",
    "# Perform grid search and evaluation with different train-test splits\n",
    "for train_size in [70, 75, 80, 85, 90]:\n",
    "    # Split the data into training and testing subsets\n",
    "    train_size_index = int(df_features.shape[0] * (train_size / 100))\n",
    "    X_train = df_features[:train_size_index]\n",
    "    X_test = df_features[train_size_index:]\n",
    "    y_train = y_labels[:train_size_index]\n",
    "    y_test = y_labels[train_size_index:]\n",
    "\n",
    "\n",
    "    # Create the GridSearchCV object\n",
    "    grid_search = GridSearchCV(svm_clf, param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "    # Perform the grid search on your data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters from the grid search\n",
    "    best_kernel = grid_search.best_params_['kernel']\n",
    "    best_C = grid_search.best_params_['C']\n",
    "    best_gamma = grid_search.best_params_['gamma']\n",
    "\n",
    "    # Train the final model with the best hyperparameters\n",
    "    final_svm_clf = SVC(kernel=best_kernel, C=best_C, gamma=best_gamma, random_state=42)\n",
    "    final_svm_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = final_svm_clf.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Train size: {train_size}, Best kernel: {best_kernel}, Best C: {best_C}, Best gamma: {best_gamma}, Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
